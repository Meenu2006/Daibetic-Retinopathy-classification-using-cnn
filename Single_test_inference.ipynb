{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/souravs17031999/Retinal_blindness_detection_Pytorch/blob/master/Single_test_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "colab": {},
        "colab_type": "code",
        "id": "aqpNhKZhyQaI",
        "outputId": "7f810a59-6171-4d42-93b9-6cd82ab55d3b",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['aptos2019-blindess-detection', 'blindness.py', 'GettingStarted.md', 'images', 'inference.ipynb', 'input', 'model.py', 'README.md', 'requirements.txt', 'sampleimages', 'send_sms.py', 'Single_test_inference.ipynb', 'submission.csv', 'training.ipynb']\n"
          ]
        }
      ],
      "source": [
        "# Imports here\n",
        "from __future__ import print_function, division\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils import data\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, models\n",
        "import torchvision.models as models\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from skimage import io, transform\n",
        "import torch.utils.data as data_utils\n",
        "from PIL import Image, ImageFile\n",
        "import json\n",
        "from torch.optim import lr_scheduler\n",
        "import time\n",
        "import os\n",
        "import argparse\n",
        "import copy\n",
        "import pandas as pd\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "import cv2\n",
        "# Import useful sklearn functions\n",
        "import sklearn\n",
        "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
        "import time\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "import os\n",
        "# print(os.listdir(\"../input\"))\n",
        "# base_dir = \"../input/aptos2019-blindness-detection/\"\n",
        "print(os.listdir(\"C:\\\\Users\\\\singh\\\\OneDrive\\\\Desktop\\\\Retinal_blindness_detection_Pytorch-master\\\\\"))\n",
        "base_dir = \"./input/C:\\\\Users\\\\singh\\\\OneDrive\\\\Desktop\\\\Retinal_blindness_detection_Pytorch-master\\\\\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "8thQ8UPsyQaM",
        "outputId": "a9812f99-f709-440c-bdfd-b93f0b5ef29e",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['sample_submission.csv', 'test.csv', 'test_images', 'train.csv', 'train_images']\n"
          ]
        }
      ],
      "source": [
        "# print(os.listdir(\"../input/kernel4f121f3247\"))\n",
        "print(os.listdir(\"C:\\\\Users\\\\singh\\\\OneDrive\\\\Desktop\\\\Retinal_blindness_detection_Pytorch-master\\\\aptos2019-blindess-detection\\\\\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "E1WjW_VUyQaa",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class CreateDataset(Dataset):\n",
        "    def __init__(self, df_data, data_dir = '../input/', transform=None):\n",
        "        super().__init__()\n",
        "        self.df = df_data.values\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        img_name,label = self.df[index]\n",
        "        img_path = os.path.join(self.data_dir, img_name+'.png')\n",
        "        image = cv2.imread(img_path)  # if getting error, use PIL to load the image here.\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "HDJRlQicyQap",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# test_csv = pd.read_csv('../input/aptos2019-blindness-detection/test.csv')\n",
        "test_csv = pd.read_csv('C:\\\\Users\\\\singh\\\\OneDrive\\\\Desktop\\\\Retinal_blindness_detection_Pytorch-master\\\\aptos2019-blindess-detection\\\\test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "gVppvdUfyQau",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "test_path = \"../input/aptos2019-blindness-detection/test_images/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "5uCeyL8CyQa7",
        "trusted": true
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'torchvision' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_transforms \u001b[39m=\u001b[39m torchvision\u001b[39m.\u001b[39mtransforms\u001b[39m.\u001b[39mCompose([\n\u001b[0;32m      2\u001b[0m     torchvision\u001b[39m.\u001b[39mtransforms\u001b[39m.\u001b[39mResize((\u001b[39m224\u001b[39m, \u001b[39m224\u001b[39m)),\n\u001b[0;32m      3\u001b[0m     \u001b[39m#torchvision.transforms.ColorJitter(brightness=2, contrast=2),\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     torchvision\u001b[39m.\u001b[39mtransforms\u001b[39m.\u001b[39mRandomHorizontalFlip(p\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m),\n\u001b[0;32m      5\u001b[0m     torchvision\u001b[39m.\u001b[39mtransforms\u001b[39m.\u001b[39mToTensor(),\n\u001b[0;32m      6\u001b[0m     torchvision\u001b[39m.\u001b[39mtransforms\u001b[39m.\u001b[39mNormalize(mean\u001b[39m=\u001b[39m(\u001b[39m0.485\u001b[39m, \u001b[39m0.456\u001b[39m, \u001b[39m0.406\u001b[39m), std\u001b[39m=\u001b[39m(\u001b[39m0.229\u001b[39m, \u001b[39m0.224\u001b[39m, \u001b[39m0.225\u001b[39m))\n\u001b[0;32m      7\u001b[0m ])\n",
            "\u001b[1;31mNameError\u001b[0m: name 'torchvision' is not defined"
          ]
        }
      ],
      "source": [
        "test_transforms = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize((224, 224)),\n",
        "    #torchvision.transforms.ColorJitter(brightness=2, contrast=2),\n",
        "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "w4EMG86xyQa9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "test_csv['diagnosis'] = -1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "H9ZYfDaCyQbJ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "test_data = CreateDataset(df_data=test_csv, data_dir=test_path, transform=test_transforms)\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "6u4PbduZyQbM",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def round_off_preds(preds, coef=[0.5, 1.5, 2.5, 3.5]):\n",
        "    for i, pred in enumerate(preds):\n",
        "            if pred < coef[0]:\n",
        "                preds[i] = 0\n",
        "            elif pred >= coef[0] and pred < coef[1]:\n",
        "                preds[i] = 1\n",
        "            elif pred >= coef[1] and pred < coef[2]:\n",
        "                preds[i] = 2\n",
        "            elif pred >= coef[2] and pred < coef[3]:\n",
        "                preds[i] = 3\n",
        "            else:\n",
        "                preds[i] = 4\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "RoFP-7rkyQba",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def predict(testloader):\n",
        "    '''Function used to make predictions on the test set'''\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    for batch_i, (data, target) in enumerate(testloader):\n",
        "        data, target = data.cuda(), target.cuda()\n",
        "        output = model(data)\n",
        "        pr = output.detach().cpu().numpy()\n",
        "        for i in pr:\n",
        "            preds.append(i.item())\n",
        "            \n",
        "    return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "8OAK8mGwyQbi",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def load_model(path):\n",
        "  checkpoint = torch.load(path)\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "K7f1z9RKyQbp",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\singh\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "C:\\Users\\singh\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = models.resnet152(pretrained=False) \n",
        "\n",
        "num_ftrs = model.fc.in_features \n",
        "out_ftrs = 5 \n",
        "  \n",
        "model.fc = nn.Sequential(nn.Linear(num_ftrs, 512),nn.ReLU(),nn.Linear(512,out_ftrs),nn.LogSoftmax(dim=1))\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = torch.optim.Adam(filter(lambda p:p.requires_grad,model.parameters()) , lr = 0.00001) \n",
        "\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "model.to(device);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "pF7lrBGyyQby",
        "outputId": "38443da0-2db4-4f4c-8ce2-50f9a35507dd",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "conv1is frozen\n",
            "bn1is frozen\n",
            "reluis frozen\n",
            "maxpoolis frozen\n",
            "layer1is frozen\n",
            "layer2is unfrozen\n",
            "layer3is unfrozen\n",
            "layer4is unfrozen\n",
            "avgpoolis frozen\n",
            "fcis unfrozen\n"
          ]
        }
      ],
      "source": [
        "# to unfreeze more layers \n",
        "for name,child in model.named_children():\n",
        "  if name in ['layer2','layer3','layer4','fc']:\n",
        "    print(name + 'is unfrozen')\n",
        "    for param in child.parameters():\n",
        "      param.requires_grad = True\n",
        "  else:\n",
        "    print(name + 'is frozen')\n",
        "    for param in child.parameters():\n",
        "      param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "sqaWCqQ6yQb-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(filter(lambda p:p.requires_grad,model.parameters()) , lr = 0.000001) \n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "kHPsRfUEyQcK",
        "trusted": true
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\singh\\\\OneDrive\\\\Desktop\\\\classifier.pt'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m load_model(\u001b[39m\"\u001b[39;49m\u001b[39mC:\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mUsers\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39msingh\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mOneDrive\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mDesktop\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mclassifier.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
            "Cell \u001b[1;32mIn[13], line 2\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_model\u001b[39m(path):\n\u001b[1;32m----> 2\u001b[0m   checkpoint \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(path)\n\u001b[0;32m      3\u001b[0m   model\u001b[39m.\u001b[39mload_state_dict(checkpoint[\u001b[39m'\u001b[39m\u001b[39mmodel_state_dict\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      4\u001b[0m   optimizer\u001b[39m.\u001b[39mload_state_dict(checkpoint[\u001b[39m'\u001b[39m\u001b[39moptimizer_state_dict\u001b[39m\u001b[39m'\u001b[39m])\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    789\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 791\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[0;32m    792\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    793\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    794\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    795\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    796\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 271\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    272\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    273\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[1;32m--> 252\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\singh\\\\OneDrive\\\\Desktop\\\\classifier.pt'"
          ]
        }
      ],
      "source": [
        "model = load_model(\"C:\\\\Users\\\\singh\\\\OneDrive\\\\Desktop\\\\classifier.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "oToTzW9UyQcP",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "test_dir = \"../input/aptos2019-blindness-detection/test_images/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "MmUx-hEryQcT",
        "outputId": "5c1a0350-f03e-449f-937a-6c99980e9640",
        "trusted": true
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Unexpected type <class 'NoneType'>",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[19], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m img_ids \u001b[39m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m----> 6\u001b[0m \u001b[39mfor\u001b[39;00m inputs, labels \u001b[39min\u001b[39;00m test_loader:\n\u001b[0;32m      7\u001b[0m     i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m      8\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "Cell \u001b[1;32mIn[3], line 16\u001b[0m, in \u001b[0;36mCreateDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     14\u001b[0m image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(img_path)  \u001b[39m# if getting error, use PIL to load the image here.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 16\u001b[0m     image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(image)\n\u001b[0;32m     17\u001b[0m \u001b[39mreturn\u001b[39;00m image, label\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\transforms\\transforms.py:361\u001b[0m, in \u001b[0;36mResize.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m    354\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[39m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[39m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mresize(img, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterpolation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mantialias)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\transforms\\functional.py:476\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[39mif\u001b[39;00m max_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(size) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    471\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    472\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mmax_size should only be passed if size specifies the length of the smaller edge, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    473\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mi.e. size should be an int or a sequence of length 1 in torchscript mode.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    474\u001b[0m         )\n\u001b[1;32m--> 476\u001b[0m _, image_height, image_width \u001b[39m=\u001b[39m get_dimensions(img)\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(size, \u001b[39mint\u001b[39m):\n\u001b[0;32m    478\u001b[0m     size \u001b[39m=\u001b[39m [size]\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\transforms\\functional.py:78\u001b[0m, in \u001b[0;36mget_dimensions\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(img, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m     76\u001b[0m     \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39mget_dimensions(img)\n\u001b[1;32m---> 78\u001b[0m \u001b[39mreturn\u001b[39;00m F_pil\u001b[39m.\u001b[39;49mget_dimensions(img)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\transforms\\_functional_pil.py:31\u001b[0m, in \u001b[0;36mget_dimensions\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m     29\u001b[0m     width, height \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39msize\n\u001b[0;32m     30\u001b[0m     \u001b[39mreturn\u001b[39;00m [channels, height, width]\n\u001b[1;32m---> 31\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnexpected type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(img)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "\u001b[1;31mTypeError\u001b[0m: Unexpected type <class 'NoneType'>"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    p_labels = []\n",
        "    img_ids = []\n",
        "    i = 0\n",
        "    for inputs, labels in test_loader:\n",
        "        i += 1\n",
        "        if i % 10 == 0:\n",
        "            print(f'{i} pass step')\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        p_labels.append(preds)\n",
        "    # getting ids of file images    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "n58Bf7aRyQch",
        "outputId": "5118c232-c777-47de-a789-d7e5b549efb5",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "p_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "fsWeTc0SyQdA",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "pred_labels = []\n",
        "for l in p_labels:\n",
        "    for l1 in l:\n",
        "        pred_labels.append(l1.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "siCP7zPkyQdm",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "sample_sub = pd.read_csv('C:\\\\Users\\\\singh\\\\OneDrive\\\\Desktop\\\\Retinal_blindness_detection_Pytorch-master\\\\aptos2019-blindess-detection/sample_submission.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "zZ2SmY-NyQd2",
        "trusted": true
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Length of values (0) does not match length of index (1928)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sample_sub\u001b[39m.\u001b[39;49mdiagnosis \u001b[39m=\u001b[39m pred_labels\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\generic.py:6018\u001b[0m, in \u001b[0;36mNDFrame.__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   6016\u001b[0m     \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(\u001b[39mself\u001b[39m, name, value)\n\u001b[0;32m   6017\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis:\n\u001b[1;32m-> 6018\u001b[0m     \u001b[39mself\u001b[39;49m[name] \u001b[39m=\u001b[39m value\n\u001b[0;32m   6019\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   6020\u001b[0m     \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(\u001b[39mself\u001b[39m, name, value)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\frame.py:3960\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3957\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   3958\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3959\u001b[0m     \u001b[39m# set column\u001b[39;00m\n\u001b[1;32m-> 3960\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_item(key, value)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\frame.py:4153\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4143\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_set_item\u001b[39m(\u001b[39mself\u001b[39m, key, value) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   4144\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4145\u001b[0m \u001b[39m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[0;32m   4146\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4151\u001b[0m \u001b[39m    ensure homogeneity.\u001b[39;00m\n\u001b[0;32m   4152\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4153\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sanitize_column(value)\n\u001b[0;32m   4155\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   4156\u001b[0m         key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\n\u001b[0;32m   4157\u001b[0m         \u001b[39mand\u001b[39;00m value\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   4158\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_extension_array_dtype(value)\n\u001b[0;32m   4159\u001b[0m     ):\n\u001b[0;32m   4160\u001b[0m         \u001b[39m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[0;32m   4161\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mis_unique \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns, MultiIndex):\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\frame.py:4880\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   4877\u001b[0m     \u001b[39mreturn\u001b[39;00m _reindex_for_setitem(Series(value), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\n\u001b[0;32m   4879\u001b[0m \u001b[39mif\u001b[39;00m is_list_like(value):\n\u001b[1;32m-> 4880\u001b[0m     com\u001b[39m.\u001b[39;49mrequire_length_match(value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex)\n\u001b[0;32m   4881\u001b[0m \u001b[39mreturn\u001b[39;00m sanitize_array(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, allow_2d\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\common.py:576\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    573\u001b[0m \u001b[39mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    574\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    575\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(index):\n\u001b[1;32m--> 576\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    577\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mLength of values \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    578\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(data)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    579\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdoes not match length of index \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    580\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(index)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    581\u001b[0m     )\n",
            "\u001b[1;31mValueError\u001b[0m: Length of values (0) does not match length of index (1928)"
          ]
        }
      ],
      "source": [
        "sample_sub.diagnosis = pred_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "QGFIu9W5yQeD",
        "outputId": "b27de66f-20ce-4127-f2d6-4e0c2126751e",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of            id_code  diagnosis\n",
              "0     0005cfc8afb6          0\n",
              "1     003f0afdcd15          0\n",
              "2     006efc72b638          0\n",
              "3     00836aaacf06          0\n",
              "4     009245722fa4          0\n",
              "...            ...        ...\n",
              "1923  ff2fd94448de          0\n",
              "1924  ff4c945d9b17          0\n",
              "1925  ff64897ac0d8          0\n",
              "1926  ffa73465b705          0\n",
              "1927  ffdc2152d455          0\n",
              "\n",
              "[1928 rows x 2 columns]>"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample_sub.head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "S9YwxsNCyQeP",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "sample_sub.to_csv('submission.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "RVctDWuOyQeY",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def test_with_single_image(model, file, transform, classes):\n",
        "\n",
        "    file = Image.open(file).convert('RGB')\n",
        "\n",
        "    img = transform(file).unsqueeze(0)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model(img.to(device))\n",
        "        ps = torch.exp(out)\n",
        "        top_p, top_class = ps.topk(1, dim=1)\n",
        "        value = top_class.item()\n",
        "        print(\"Value:\", value)\n",
        "        print(classes[value])\n",
        "        plt.imshow(np.array(file))\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "KLWNzaJLyQeo",
        "outputId": "e80b5a7f-5494-4ad9-b1bd-510415cd6479",
        "trusted": true
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "[WinError 123] The filename, directory name, or volume label syntax is incorrect: '../input/C:\\\\Users\\\\singh\\\\OneDrive\\\\Desktop\\\\Retinal_blindness_detection_Pytorch-master'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[32], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m test_dir \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m../input/C:\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mUsers\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39msingh\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mOneDrive\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mDesktop\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mRetinal_blindness_detection_Pytorch-master\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[39m# take random folder first\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m folders \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mlistdir(test_dir)\n\u001b[0;32m      6\u001b[0m num \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m      7\u001b[0m path \u001b[39m=\u001b[39m test_dir\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mfolders[num]\n",
            "\u001b[1;31mOSError\u001b[0m: [WinError 123] The filename, directory name, or volume label syntax is incorrect: '../input/C:\\\\Users\\\\singh\\\\OneDrive\\\\Desktop\\\\Retinal_blindness_detection_Pytorch-master'"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "test_dir = \"../input/C:\\\\Users\\\\singh\\\\OneDrive\\\\Desktop\\\\Retinal_blindness_detection_Pytorch-master\"\n",
        "# take random folder first\n",
        "folders = os.listdir(test_dir)\n",
        "num = 2\n",
        "path = test_dir+\"/\"+folders[num]\n",
        "print(path)\n",
        "# save label\n",
        "label = folders[num]\n",
        "\n",
        "# now take random file\n",
        "files = os.listdir(path)\n",
        "num = random.randint(0, len(files)-1)\n",
        "name = path + \"/\" + files[num]\n",
        "print('Path: ', name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Nvcd6JNPyQe4",
        "outputId": "9f743824-3d58-4b9a-d81c-04dffc9d2b36",
        "trusted": true
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'fc'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[33], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m classes \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mNo DR\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMild\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mModerate\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSevere\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mProliferative DR\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m test_with_single_image(model, name, test_transforms, classes)\n",
            "Cell \u001b[1;32mIn[28], line 3\u001b[0m, in \u001b[0;36mtest_with_single_image\u001b[1;34m(model, file, transform, classes)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtest_with_single_image\u001b[39m(model, file, transform, classes):\n\u001b[1;32m----> 3\u001b[0m     file \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mopen(file)\u001b[39m.\u001b[39mconvert(\u001b[39m'\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m     img \u001b[39m=\u001b[39m transform(file)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m      7\u001b[0m     device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\PIL\\Image.py:3236\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3233\u001b[0m     filename \u001b[39m=\u001b[39m fp\n\u001b[0;32m   3235\u001b[0m \u001b[39mif\u001b[39;00m filename:\n\u001b[1;32m-> 3236\u001b[0m     fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   3237\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   3239\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'fc'"
          ]
        }
      ],
      "source": [
        "classes = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferative DR']\n",
        "test_with_single_image(model, name, test_transforms, classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "Single_test_inference.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
